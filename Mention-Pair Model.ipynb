{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'input_data.csv'\n",
    "raw_data = open(filename, 'rt')\n",
    "data = np.loadtxt(raw_data, delimiter= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426088, 1148)\n"
     ]
    }
   ],
   "source": [
    "print data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_features = len(data[0])-1\n",
    "size_of_dataset = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_features = data[:,0:number_of_features]\n",
    "labels = data[:,number_of_features] #last column consists of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split data into trainig and validation set (90/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) #seed fixed for reproducibility\n",
    "mask = np.random.rand(size_of_dataset) < 0.9  #array of boolean variables\n",
    "\n",
    "training_set = input_features[mask]\n",
    "training_labels = labels[mask]\n",
    "\n",
    "validation_set = input_features[~mask]\n",
    "validation_labels = labels[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(number_of_features,))\n",
    "\n",
    "output_from_1st_layer = Dense(1000, activation='relu')(inputs)\n",
    "output_from_1st_layer = Dropout(0.2)(output_from_1st_layer)\n",
    "output_from_1st_layer = BatchNormalization()(output_from_1st_layer)\n",
    "\n",
    "output_from_2nd_layer = Dense(500, activation='relu')(output_from_1st_layer)\n",
    "output_from_2nd_layer = Dropout(0.2)(output_from_2nd_layer)\n",
    "output_from_2nd_layer = BatchNormalization()(output_from_2nd_layer)\n",
    "\n",
    "\n",
    "output_from_3rd_layer = Dense(300, activation='relu')(output_from_2nd_layer)\n",
    "output_from_3rd_layer = Dropout(0.2)(output_from_3rd_layer)\n",
    "output_from_3rd_layer = BatchNormalization()(output_from_3rd_layer)\n",
    "\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(output_from_3rd_layer)\n",
    "\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Model compilation with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1147)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1148000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 1,806,301\n",
      "Trainable params: 1,802,701\n",
      "Non-trainable params: 3,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawelmorawiecki/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426088 samples, validate on 42445 samples\n",
      "Epoch 1/3\n",
      "426088/426088 [==============================] - 169s - loss: 0.4197 - acc: 0.7983 - val_loss: 0.3137 - val_acc: 0.8597\n",
      "Epoch 2/3\n",
      "426088/426088 [==============================] - 156s - loss: 0.3150 - acc: 0.8579 - val_loss: 0.2469 - val_acc: 0.8957\n",
      "Epoch 3/3\n",
      "426088/426088 [==============================] - 141s - loss: 0.2527 - acc: 0.8893 - val_loss: 0.1841 - val_acc: 0.9251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2099064d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_set, training_labels, batch_size=256, nb_epoch=3, \n",
    "          validation_data=(validation_set, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model_1147_features.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"model_1147_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'test_set.csv'\n",
    "raw_data = open(filename, 'rt')\n",
    "test_data = np.loadtxt(raw_data, delimiter= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = test_data[:,0:number_of_features]\n",
    "test_labels = test_data[:,number_of_features] #last column consists of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40764, 1147)\n"
     ]
    }
   ],
   "source": [
    "print test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40512/40764 [============================>.] - ETA: 0sacc: 77.27%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_set, test_labels, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99831080\n"
     ]
    }
   ],
   "source": [
    "single_example = test_set[4:5,:] #example number 5 from the test set\n",
    "prediction = model.predict(single_example)\n",
    "print '%.8f' % prediction[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
